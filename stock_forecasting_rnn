# -*- coding: utf-8 -*-
"""Marcello H. Martins - trabalho_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10iIuXJt-CCyBLL-1mnO-USFYWGGQL8LN

Trabalho RNN - Deep Learning
Marcello Henrique Martins
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function, unicode_literals
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf

from keras.callbacks import EarlyStopping

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd

mpl.rcParams['figure.figsize'] = (8, 6)
mpl.rcParams['axes.grid'] = False

base = '/content/drive/MyDrive/Data Science - pós/Deep Learning/Aula 5 - RNN e LSTM/Material trabalho/Base de Ações Trabalho - Página1.csv'

df = pd.read_csv(base)
df.drop(index = 0, axis=0, inplace=True)
df.head(2)

df1 = df[['PETR4', 'Unnamed: 4']]
df1.rename(columns={'Unnamed: 4':'valor'}, inplace=True)
df1['PETR4'] = pd.to_datetime(df1['PETR4']).dt.date # datetime para date
df1.dropna(inplace=True)
df1.head(2)

df1.describe

data = df1['valor']

data.index = df1['PETR4']

print ("Numero de Amostras: ", len(data))
print ("Vetor de valores:" , data.values)
data.head()

data = data.apply(lambda x: x.replace(',','.')) # trocar a vírgula por ponto

data = data1.apply(lambda x: float(x)) # string to float

data.plot(subplots=True)

TRAIN_SPLIT = 500
tf.random.set_seed(13)

uni_data = data.values
print ("Dados: ", uni_data)
uni_train_mean = uni_data[:TRAIN_SPLIT].mean()
print ("Média: ", uni_train_mean)
uni_train_std = uni_data[:TRAIN_SPLIT].std()
print ("Desv. Padrão: ", uni_train_std)
uni_data = (uni_data-uni_train_mean)/uni_train_std
print ("Dados Norm: ", uni_data)

"""A função abaixo retorna a fração do dataset a ser utilizada sendo:
history size
target size
"""

def univariate_data(dataset, start_index, end_index, history_size, target_size):
  data = []
  labels = []

  start_index = start_index + history_size
  if end_index is None:
    end_index = len(dataset) - target_size

  for i in range(start_index, end_index):
    indices = range(i-history_size, i)
    # Reshape data from (history_size,) to (history_size, 1)
    data.append(np.reshape(dataset[indices], (history_size, 1)))
    labels.append(dataset[i+target_size])
  return np.array(data), np.array(labels)

'''Tamanho da Janela do Historico'''
univariate_past_history = 100  # observacoes anteriores (número de observações anteriores que usa para fazer a previsão)
future = univariate_future_target = 1  #a proxima observação (quantas observações à frente quero prever?)

x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,
                                           univariate_past_history,
                                           univariate_future_target)
x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,
                                       univariate_past_history,
                                       univariate_future_target)

def create_time_steps(length):
  time_steps = []
  for i in range(-length, 0, 1):
    time_steps.append(i)
  return time_steps

def show_plot(plot_data, delta, title):
    labels = ['History', 'True Future', 'Model Prediction']
    marker = ['.-', 'gX', 'ro']
    time_steps = create_time_steps(plot_data[0].shape[0])
    if delta:
      future = delta
    else:
      future = 0

    plt.title(title)
    for i, x in enumerate(plot_data):
      if i:
        plt.plot(future, plot_data[i], marker[i], markersize=10,
                label=labels[i])
      else:
        plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
    plt.legend()
    plt.xlim([time_steps[0], (future+5)*2])
    plt.xlabel('Time-Step')
    return plt

sample_id = 5
print("#Amostras:", len(x_train_uni),"#Labels: ", len(y_train_uni))
print ("Amostra[0]:\n", x_train_uni[sample_id],"\nValor: ", y_train_uni[sample_id])
show_plot([x_train_uni[sample_id], y_train_uni[sample_id]], future, 'Sample Example')

def baseline(history):
  return np.mean(history)

show_plot([x_train_uni[sample_id], y_train_uni[sample_id], baseline(x_train_uni[sample_id])], 0,
           'Baseline Prediction Example')

"""RNN"""

BATCH_SIZE = 64
BUFFER_SIZE = 100

train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))
train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()

val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))
val_univariate = val_univariate.batch(BATCH_SIZE).repeat()

"""Criando as arquiteturas"""

simple_rnn_model = tf.keras.models.Sequential([
    tf.keras.layers.SimpleRNN(4, input_shape=(x_train_uni.shape[1], 
    x_train_uni.shape[2])),    # return_sequences=True pode ser param
    tf.keras.layers.Dense(1)
])


simple_rnn_model.compile(optimizer='adam', loss='mae')

simple_rnn_model.summary()

EVALUATION_INTERVAL = len(data)
EPOCHS = 10
es = EarlyStopping(monitor='val_loss')

rnn_log = simple_rnn_model.fit(train_univariate, epochs=EPOCHS,
                      steps_per_epoch=EVALUATION_INTERVAL,
                      validation_data=val_univariate, validation_steps=50, callbacks=[es])

def plot_train_history(history, title):
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(len(loss))

  plt.figure()

  plt.plot(epochs, loss, 'b', label='Training loss')
  plt.plot(epochs, val_loss, 'r', label='Validation loss')
  plt.title(title)
  plt.legend()

  plt.show()

plot_train_history(rnn_log,
                   'RNN Training and validation loss')

def plot_preds(plot_data, delta=0):
    labels = ['History', 'True Future', 'RNN Prediction']
    marker = ['.-', 'gX', 'ro']
    time_steps = create_time_steps(plot_data[0].shape[0])
    

    future = delta

    plt.title('Predictions')
    for i, x in enumerate(plot_data):
      if i:
        plt.plot(future, plot_data[i], marker[i], markersize=10,
                label=labels[i])
      else:
        plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
    plt.legend()
    plt.xlim([time_steps[0], (future+5)*2])
    plt.xlabel('Time-Step')
    return plt

for x, y in val_univariate.take(5):
  plot = plot_preds([x[0].numpy(), y[0].numpy(),
                    simple_rnn_model.predict(x)[0]], future)
  plot.show()

#TODO Calcule uma taxa de acerto

err_rnn=0

for x, y in val_univariate.take(10):
  err_rnn += abs(y[0].numpy() - simple_rnn_model.predict(x)[0])
  
err_rnn = err_rnn/10
acerto = 1 - err_rnn
  
print(f'erro: {err_rnn}, acerto: {acerto}')





"""Teste com dados sem normalizar

data_raw = data.values 

def univariate_data(dataset, start_index, end_index, history_size, target_size):
  data = []
  labels = []

  start_index = start_index + history_size
  if end_index is None:
    end_index = len(dataset) - target_size

  for i in range(start_index, end_index):
    indices = range(i-history_size, i)
    # Reshape data from (history_size,) to (history_size, 1)
    data.append(np.reshape(dataset[indices], (history_size, 1)))
    labels.append(dataset[i+target_size])
  return np.array(data), np.array(labels)


  '''Tamanho da Janela do Historico'''
univariate_past_history = 20  #20 observacoes anteriores (número de observações anteriores que usa para fazer a previsão)
future = univariate_future_target = 1  #a proxima observação (quantas observações à frente quero prever?)

x_train_uni, y_train_uni = univariate_data(data_raw, 0, TRAIN_SPLIT,
                                           univariate_past_history,
                                           univariate_future_target)
x_val_uni, y_val_uni = univariate_data(data_raw, TRAIN_SPLIT, None,
                                       univariate_past_history,
                                       univariate_future_target)



def create_time_steps(length):
  time_steps = []
  for i in range(-length, 0, 1):
    time_steps.append(i)
  return time_steps

def show_plot(plot_data, delta, title):
    labels = ['History', 'True Future', 'Model Prediction']
    marker = ['.-', 'gX', 'ro']
    time_steps = create_time_steps(plot_data[0].shape[0])
    if delta:
      future = delta
    else:
      future = 0

    plt.title(title)
    for i, x in enumerate(plot_data):
      if i:
        plt.plot(future, plot_data[i], marker[i], markersize=10,
                label=labels[i])
      else:
        plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
    plt.legend()
    plt.xlim([time_steps[0], (future+5)*2])
    plt.xlabel('Time-Step')
    return plt

sample_id = 5
print("#Amostras:", len(x_train_uni),"#Labels: ", len(y_train_uni))
print ("Amostra[0]:\n", x_train_uni[sample_id],"\nValor: ", y_train_uni[sample_id])
show_plot([x_train_uni[sample_id], y_train_uni[sample_id]], future, 'Sample Example')

def baseline(history):
  return np.mean(history)

show_plot([x_train_uni[sample_id], y_train_uni[sample_id], baseline(x_train_uni[sample_id])], 0,
           'Baseline Prediction Example')


BATCH_SIZE = 64
BUFFER_SIZE = 100

train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))
train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()

val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))
val_univariate = val_univariate.batch(BATCH_SIZE).repeat()

simple_rnn_model = tf.keras.models.Sequential([
    tf.keras.layers.SimpleRNN(8, input_shape=(x_train_uni.shape[1], 
    x_train_uni.shape[2]), activation='relu'),    # return_sequences=True pode ser param
    tf.keras.layers.Dense(1)
])


simple_rnn_model.compile(optimizer='adam', loss='mae')

simple_rnn_model.summary()

EVALUATION_INTERVAL = len(data)
EPOCHS = 10
es = EarlyStopping(monitor='val_loss')

rnn_log = simple_rnn_model.fit(train_univariate, epochs=EPOCHS,
                      steps_per_epoch=EVALUATION_INTERVAL,
                      validation_data=val_univariate, validation_steps=50, callbacks=[es])

def plot_train_history(history, title):
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(len(loss))

  plt.figure()

  plt.plot(epochs, loss, 'b', label='Training loss')
  plt.plot(epochs, val_loss, 'r', label='Validation loss')
  plt.title(title)
  plt.legend()

  plt.show()

  plot_train_history(rnn_log,
                   'RNN Training and validation loss')
   
def plot_preds(plot_data, delta=0):
  labels = ['History', 'True Future', 'RNN Prediction']
  marker = ['.-', 'gX', 'ro']
  time_steps = create_time_steps(plot_data[0].shape[0])
    

  future = delta

  plt.title('Predictions')
  for i, x in enumerate(plot_data):
    if i:
      plt.plot(future, plot_data[i], marker[i], markersize=10,
                label=labels[i])
    else:
      plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
  plt.legend()
  plt.xlim([time_steps[0], (future+5)*2])
  plt.xlabel('Time-Step')
  return plt

for x, y in val_univariate.take(5):
  plot = plot_preds([x[0].numpy(), y[0].numpy(),
                    simple_rnn_model.predict(x)[0]], future)
  plot.show()

  #TODO Calcule uma taxa de acerto

err_rnn=0

for x, y in val_univariate.take(10):
  err_rnn += abs(y[0].numpy() - simple_rnn_model.predict(x)[0])
  
err_rnn = err_rnn/10
acerto = 1-err_rnn
  
print(f'erro: {err_rnn}, acerto: {acerto}')
"""
"""
